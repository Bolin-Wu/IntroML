\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\begin{document}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Introduction to Machine Learning, Big Data, and AI}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 1}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

\input{general_info.tex}

\HRule

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)

<<echo=FALSE, eval=TRUE>>=
options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@


\newpage


\section{Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent}

This assignment will study different ways to optimize common objective functions in many areas of Machine Learning, namely, stochastic gradient descent. Here we will test to implement these optimizers for a well-known model, logistic regression.

We are going to work with this data as at test case:

<<echo=TRUE,eval=FALSE>>=
library(uuml)
data("binary")

binary$gre_sd <- (binary$gre - mean(binary$gre))/sd(binary$gre)
binary$gpa_sd <- (binary$gpa - mean(binary$gpa))/sd(binary$gpa)
X <- model.matrix(admit ~ gre_sd + gpa_sd, binary)
y <- binary$admit
@


\subsection{Implement the gradient for logistic regression}


The likelihood function for logistic regression is
\[
L(\theta, \mathbf{y}, \mathbf{X}) = \prod^n_{i=1} p_i^{y_i} (1 - p_i)^{1-y_i} \,,
\]
where
\[
\log \frac{p_i}{1-p_i} = \mathbf{x}_i \theta  \,,
\]
and $\mathbf{x}_i$ is the $i$th row from the design matrix $\mathbf{X}$ and $\theta \in \mathbb{R}^P$ is a $1 \times P$ matrix with the parameters of interest.

Commonly, to find maximum likelihood estimates of $\theta$ we usually use the log likelihood as the objective function we want to optimize, i.e.:
\begin{align}
l(\theta, \mathbf{y}, \mathbf{X}) = & \sum_{i=1}^n y_i \log(p_i) + (1 - y_i) \log(1-p_i)\\
 = & \sum_{i=1}^n y_i \mathbf{x}_i\theta + \log(1-p_i)\\
 = & \sum_{i=1}^n y_i \mathbf{x}_i\theta - \log(1+\exp(\mathbf{x}_i\theta))\,.
\end{align}

Although, in our case we instead want to minimize the negative log likelihood NLL$(\theta, \mathbf{y}, \mathbf{X})=-l(\theta, \mathbf{y}, \mathbf{X})$.

\begin{enumerate}
\item Derive the the gradient for NLL$(\theta, \mathbf{y}, \mathbf{X})$ with respect to $\theta$.
\item Implement the gradient as a function in R. Below are two examples of how it should work. Note, \texttt{l\_grad(y, X, theta)} return the gradient of $(1/n) l(\theta, \mathbf{y}, \mathbf{X})$.
<<echo=FALSE,eval=TRUE>>=
l_grad <- function(y, X, theta){
c("(Intercept)" = -0.1825, "gre_sd" = 0.0857, "gpa_sd" = 0.0829)
}
@
<<echo=TRUE,eval=TRUE>>=
l_grad(y, X, theta = c(0,0,0))
@

<<echo=FALSE,eval=TRUE>>=
l_grad <- function(y, X, theta){
c("(Intercept)" = 0.0217, "gre_sd" = -0.0395, "gpa_sd" = -0.0426)
}
@
<<echo=TRUE,eval=TRUE>>=
l_grad(y, X, theta = c(-1,0.5,0.5))
@

\end{enumerate}

\subsection{Implement Gradient Descent}

We now have the primary tool for implementing gradient descent and stochastic gradient descent.

\begin{enumerate}
\item Implement the log likelihood $l$ or the negative log likelihood NLL in R.
<<echo=FALSE,eval=TRUE>>=
l <- function(y, X, theta){
-277.2589
}
@
<<echo=TRUE,eval=TRUE>>=
l(y, X, theta = c(0,0,0))
@

<<echo=FALSE,eval=TRUE>>=
l <- function(y, X, theta){
-244.5342
}
@
<<echo=TRUE,eval=TRUE>>=
l(y, X, theta = c(-1,0.5,0.5))
@
\item Run logistic regression in R to get an MLE estimate of \texttt{theta}.
\item Implement the following gradient descent algorithms:
\begin{enumerate}
\item ordinary (full/batch) gradient descent
\item stochastic gradient descent
\item mini-batch (stochastic) gradient descent using ten samples to estimate the gradient
\end{enumerate}
\item Try different learning parameters $\eta$. When does the algorithm converge or diverge? Visualize the iterations (x-axis) and the log-likelihood (y-axis). Show at least one plot per algorithm that converges. Describe your conclusions. Show the code (loop) you use.
\item Try to run Stochastic or mini-batch Gradient Descent with a fixed $\eta$. Try to judge when the algorithm roughly has converged and visualize 500 iterations of $\theta_t$ after the algorithm has roughly converged as a histogram. What distribution does it look like?
\end{enumerate}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
